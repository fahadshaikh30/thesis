{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\fashaikh\\Desktop\\Thesis main\\selected_data\\selectedData23.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'rawContent', 'id', 'category'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"category_y\": \"category\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(r\"C:\\Users\\fashaikh\\Desktop\\Thesis main\\selected_data\\selectedData30.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'date', 'rawContent', 'renderedContent', 'id', 'user',\n",
       "       'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
       "       'conversationId', 'lang', 'source', 'sourceUrl', 'sourceLabel', 'links',\n",
       "       'media', 'retweetedTweet', 'quotedTweet', 'inReplyToTweetId',\n",
       "       'inReplyToUser', 'mentionedUsers', 'coordinates', 'place', 'hashtags',\n",
       "       'cashtags', 'card', 'viewCount', 'vibe', 'bookmarkCount', 'filename',\n",
       "       'filepath'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filepath\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-05-01_2020-06-01_top.parquet    94\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-07-01_2020-08-01_top.parquet    93\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-02-01_2021-03-01_top.parquet    92\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-03-01_2021-04-01_top.parquet    91\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-11-01_2020-12-01_top.parquet    90\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-08-01_2020-09-01_top.parquet    87\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-06-01_2022-07-01_top.parquet    86\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-10-01_2021-11-01_top.parquet    86\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-06-01_2020-07-01_top.parquet    86\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-11-01_2022-12-01_top.parquet    86\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-09-01_2022-10-01_top.parquet    86\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-05-01_2022-06-01_top.parquet    85\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-04-01_2021-05-01_top.parquet    85\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-08-01_2022-09-01_top.parquet    84\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-10-01_2022-11-01_top.parquet    84\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-10-01_2020-11-01_top.parquet    83\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-12-01_2021-01-01_top.parquet    83\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-01-01_2022-02-01_top.parquet    82\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-07-01_2022-08-01_top.parquet    82\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-08-01_2021-09-01_top.parquet    82\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2020-09-01_2020-10-01_top.parquet    81\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-06-01_2021-07-01_top.parquet    81\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-11-01_2021-12-01_top.parquet    81\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-03-01_2022-04-01_top.parquet    81\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-05-01_2021-06-01_top.parquet    80\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-09-01_2021-10-01_top.parquet    80\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-12-01_2022-01-01_top.parquet    78\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-04-01_2022-05-01_top.parquet    78\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-01-01_2021-02-01_top.parquet    76\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-12-01_2023-01-01_top.parquet    76\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2022-02-01_2022-03-01_top.parquet    72\n",
       "/Users/fahad/Desktop/data/Thesis data/Top/BLM_2021-07-01_2021-08-01_top.parquet    67\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['filepath'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['FilePath'].apply(lambda x: os.path.basename(os.path.dirname(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['FilePath'].apply(lambda path: path.split(\"/\")[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = \"Big Cities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Big Cities    2658\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"date\", \"rawContent\", \"id\", \"category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].apply(lambda value: re.search(r'BLM_(.*?)_\\d{4}-\\d{2}-\\d{2}_\\d{4}-\\d{2}-\\d{2}\\.parquet', value).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['filename'].str.extract(r'BLM_(.*?)_\\d{4}-\\d{2}-\\d{2}_\\d{4}-\\d{2}-\\d{2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].str.replace('County_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['filename'].str.replace('BLM_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_df = pd.read_csv('County-Type-Share.csv')\n",
    "county_df.drop(columns=['county_cleaned', 'county_state', 'State', 'Type Number', 'FIPS', 'Type Number2', 'Key', 'New Names' ], inplace=True)\n",
    "new_df = df.merge(county_df, how='left', left_on='category', right_on='County')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_y\n",
       "Graying America           791185\n",
       "Rural Middle America      252249\n",
       "Evangelical Hubs          216134\n",
       "College Towns             140069\n",
       "Hispanic Centers           68057\n",
       "Exurbs                     66065\n",
       "Middle Suburbs             48035\n",
       "Urban Suburbs              38060\n",
       "Working Class Country      33093\n",
       "Big Cities                 30042\n",
       "African American South     26094\n",
       "Aging Farmlands             8056\n",
       "Military Posts              4025\n",
       "LDS Enclaves                  18\n",
       "Native American Lands          9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['category_y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['category_y'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['category_y'].fillna('Big Cities', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df[[\"date\", \"rawContent\", \"id\", \"category_y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(r\"C:\\Users\\fashaikh\\Desktop\\Thesis main\\selected_data\\selectedData32.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'C:\\Users\\fashaikh\\Desktop\\Thesis main\\combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rawContent</th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30 19:13:04+00:00</td>\n",
       "      <td>@ohmybeautybank skintype : combi to dry, oily ...</td>\n",
       "      <td>1608904017383653377</td>\n",
       "      <td>African American South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30 13:17:11+00:00</td>\n",
       "      <td>Really sitting with these inquiry questions th...</td>\n",
       "      <td>1608814457924513792</td>\n",
       "      <td>African American South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30 02:40:41+00:00</td>\n",
       "      <td>@IanVRowe Narrative feeds Democrats propaganda...</td>\n",
       "      <td>1608654278281023488</td>\n",
       "      <td>African American South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30 02:32:16+00:00</td>\n",
       "      <td>@Jack_Royston @cbouzy \"Criticized\". Ms. Weiss ...</td>\n",
       "      <td>1608652157020147712</td>\n",
       "      <td>African American South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30 01:28:06+00:00</td>\n",
       "      <td>@RepMTG LIAR!!! PROPAGANDA!!! Only domestic te...</td>\n",
       "      <td>1608636011390840832</td>\n",
       "      <td>African American South</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   \n",
       "0 2022-12-30 19:13:04+00:00  \\\n",
       "1 2022-12-30 13:17:11+00:00   \n",
       "2 2022-12-30 02:40:41+00:00   \n",
       "3 2022-12-30 02:32:16+00:00   \n",
       "4 2022-12-30 01:28:06+00:00   \n",
       "\n",
       "                                          rawContent                   id   \n",
       "0  @ohmybeautybank skintype : combi to dry, oily ...  1608904017383653377  \\\n",
       "1  Really sitting with these inquiry questions th...  1608814457924513792   \n",
       "2  @IanVRowe Narrative feeds Democrats propaganda...  1608654278281023488   \n",
       "3  @Jack_Royston @cbouzy \"Criticized\". Ms. Weiss ...  1608652157020147712   \n",
       "4  @RepMTG LIAR!!! PROPAGANDA!!! Only domestic te...  1608636011390840832   \n",
       "\n",
       "                 category  \n",
       "0  African American South  \n",
       "1  African American South  \n",
       "2  African American South  \n",
       "3  African American South  \n",
       "4  African American South  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Big Cities                6942499\n",
       "Graying America           4731866\n",
       "Rural Middle America      1557254\n",
       "Exurbs                    1341199\n",
       "Evangelical Hubs          1019493\n",
       "College Towns              939110\n",
       "Middle Suburbs             299360\n",
       "Hispanic Centers           299139\n",
       "Urban Suburbs              249073\n",
       "Working Class Country      168364\n",
       "African American South      96221\n",
       "Aging Farmlands             68476\n",
       "Military Posts              24460\n",
       "Native American Lands       11830\n",
       "LDS Enclaves                 6899\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fashaikh\\AppData\\Local\\Temp\\ipykernel_15756\\4288224649.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[df['date'] >= '2022-12-01'].drop_duplicates('id', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df[df['date'] >= '2022-12-01'].drop_duplicates('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['date'] >= '2022-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fashaikh\\AppData\\Local\\Temp\\ipykernel_15756\\3313486687.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop_duplicates('id', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "filtered_df.drop_duplicates('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] < '2022-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,filtered_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(r'C:\\Users\\fashaikh\\Desktop\\Thesis main\\combined_data.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'C:\\Users\\fashaikh\\Desktop\\Thesis main\\cleaned_data_sentiment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(r'C:\\Users\\fashaikh\\Desktop\\Thesis main\\partitioned_data', partition_cols='category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-11 15:29:41 WARNING  Found cached dataset tweet_eval (C:/Users/fashaikh/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 750.01it/s]\n",
      "2023-06-11 15:29:41 INFO     TrainerTextClassification: cardiffnlp/twitter-roberta-base-sentiment-latest, DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 45615\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12284\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-06-11 15:29:46 WARNING  Loading cached processed dataset at C:\\Users\\fashaikh\\.cache\\huggingface\\datasets\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-79ea2d3ce58ce655.arrow\n",
      "2023-06-11 15:29:47 WARNING  Loading cached processed dataset at C:\\Users\\fashaikh\\.cache\\huggingface\\datasets\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-66a2234ecbd1eab7.arrow\n",
      "2023-06-11 15:29:49 WARNING  Loading cached processed dataset at C:\\Users\\fashaikh\\.cache\\huggingface\\datasets\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-d566b1b47e5022a0.arrow\n",
      "c:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\tweetnlp\\text_classification\\trainer.py:94: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_accuracy = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tweetnlp\n",
    "from pprint import pprint\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# set language model and task\n",
    "language_model = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "task = \"sentiment\"\n",
    "\n",
    "# load dataset\n",
    "dataset, label_to_id = tweetnlp.load_dataset(task)\n",
    "\n",
    "# load trainer\n",
    "trainer_class = tweetnlp.load_trainer(task)\n",
    "\n",
    "# define trainer\n",
    "trainer = trainer_class(\n",
    "    language_model=language_model,\n",
    "    dataset=dataset,\n",
    "    label_to_id=label_to_id,\n",
    "    max_length=128,\n",
    "    split_train='train',\n",
    "    split_test='test',\n",
    "    output_dir=r'C:\\Users\\fashaikh\\Desktop\\Thesis main'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:29:53 WARNING  Loading cached shuffled indices for dataset at C:\\Users\\fashaikh\\.cache\\huggingface\\datasets\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-25e4fe137f6cca7c.arrow\n",
      "2023-06-11 15:29:53 WARNING  setup trainer without hyperparameter tuning. (provide `split_validation` for hyperparameter search)\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45615\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17106\n",
      "  0%|          | 26/17106 [00:10<1:40:53,  2.82it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(down_sample_size_train\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, ray_result_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mray_results/test\u001b[39;49m\u001b[39m\"\u001b[39;49m, parallel_cpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# save model checkpoint\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\tweetnlp\\text_classification\\trainer.py:228\u001b[0m, in \u001b[0;36mTrainerTextClassification.train\u001b[1;34m(self, output_dir, random_seed, eval_step, n_trials, split_train, split_validation, parallel_cpu, search_range_lr, search_range_epoch, search_list_batch, ray_result_dir, down_sample_size_train, down_sample_size_validation, training_arguments)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer, \u001b[39m\"\u001b[39m\u001b[39mtrain_dataset\u001b[39m\u001b[39m\"\u001b[39m, full_train_dataset)\n\u001b[0;32m    227\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39margs, \u001b[39m\"\u001b[39m\u001b[39mevaluation_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 228\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    229\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mtraining finished\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n",
      "File \u001b[1;32mc:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fashaikh\\Desktop\\Thesis main\\thesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:1745\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1740\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1743\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1744\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m-> 1745\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[0;32m   1746\u001b[0m ):\n\u001b[0;32m   1747\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1749\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train(down_sample_size_train=1000, ray_result_dir=\"ray_results/test\", parallel_cpu=True)\n",
    "\n",
    "# save model checkpoint\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "trainer.predict('If you wanna look like a badass, have drama on social media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
