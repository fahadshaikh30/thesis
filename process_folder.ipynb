{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_parquet(folder_path, output_file_path, output_file_name):\n",
    "    folder_path = Path(folder_path)\n",
    "    output_file_path = Path(output_file_path)\n",
    "\n",
    "    folder_name = folder_path.name\n",
    "\n",
    "    # Get a list of all folders in the main folder\n",
    "    subfolders = [f for f in folder_path.iterdir() if f.is_dir()]\n",
    "\n",
    "    # Initialize lists to store folder information\n",
    "    folders_with_files = []\n",
    "    folders_without_files = []\n",
    "\n",
    "    # Initialize a list to store the DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over each subfolder\n",
    "    for subfolder in subfolders:\n",
    "        # Get a list of all Parquet files in the subfolder\n",
    "        parquet_files = list(subfolder.glob(\"*.parquet\"))\n",
    "\n",
    "        if parquet_files:\n",
    "            # Subfolder contains parquet files\n",
    "            folders_with_files.append(subfolder.name)\n",
    "            for file in parquet_files:\n",
    "                # Append the DataFrame to the list\n",
    "                df = pd.read_parquet(file)\n",
    "                df[\"filename\"] = file.stem\n",
    "                df[\"filepath\"] = str(file)\n",
    "\n",
    "                # Convert the 'quotedTweet' column to nullable integer type\n",
    "                df[\"quotedTweet\"] = (\n",
    "                    df[\"quotedTweet\"]\n",
    "                    .apply(lambda x: x if isinstance(x, int) else pd.NA)\n",
    "                    .astype(pd.Int64Dtype())\n",
    "                )\n",
    "                df[\"inReplyToUser\"] = (\n",
    "                    df[\"inReplyToUser\"]\n",
    "                    .apply(lambda x: x if isinstance(x, int) else pd.NA)\n",
    "                    .astype(pd.Int64Dtype())\n",
    "                )\n",
    "                dfs.append(df)\n",
    "\n",
    "        else:\n",
    "            # Empty subfolder\n",
    "            folders_without_files.append(subfolder.name)\n",
    "\n",
    "    if dfs:\n",
    "        # Concatenate all DataFrames into a single DataFrame\n",
    "        combined_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Write the combined data to a Parquet file\n",
    "        combined_data.to_parquet(output_file_path / output_file_name, index=False)\n",
    "\n",
    "    else:\n",
    "        # No parquet files found\n",
    "        return None\n",
    "\n",
    "    # Create folder information list\n",
    "    folder_info = []\n",
    "    for folder in folders_with_files:\n",
    "        num_files = sum(folder_name in str(file) for file in parquet_files)\n",
    "        folder_info.append([folder_name, folder, num_files])\n",
    "\n",
    "    # Save folder information as a CSV file\n",
    "    folder_info_df = pd.DataFrame(folder_info, columns=[\"Main Folder\", \"Subfolder\", \"File Count\"])\n",
    "    folder_info_df.to_csv(output_file_path / \"folder_info.csv\", index=False)\n",
    "\n",
    "    return folder_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path where the Parquet files are located\n",
    "folder_path = \"/Users/fahad/Desktop/Data/More data\"\n",
    "\n",
    "# Define the output file path for the combined Parquet file\n",
    "output_file_path = \"/Users/fahad/Desktop/processed_data\"\n",
    "\n",
    "# Define the output file name for the combined Parquet file\n",
    "output_file_name = \"combined_data9.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Failed to open local file '/Users/fahad/Desktop/processed_data/combined_data9.parquet'. Detail: [errno 21] Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m folder \u001b[39m=\u001b[39m combine_parquet(folder_path, output_file_path, output_file_name)\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mcombine_parquet\u001b[0;34m(folder_path, output_file_path, output_file_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m     combined_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m     \u001b[39m# Write the combined data to a Parquet file\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     combined_data\u001b[39m.\u001b[39;49mto_parquet(output_file_path \u001b[39m/\u001b[39;49m output_file_name, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# No parquet files found\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2890\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2891\u001b[0m     path,\n\u001b[1;32m   2892\u001b[0m     engine,\n\u001b[1;32m   2893\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2894\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2895\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2896\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2898\u001b[0m )\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    190\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:3084\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3082\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   3083\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3085\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   3086\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   3087\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   3088\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m   3089\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   3090\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   3091\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[1;32m   3092\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[1;32m   3093\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[1;32m   3094\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3095\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[1;32m   3096\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   3097\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   3098\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   3099\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   3100\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   3101\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   3102\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   3103\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   3104\u001b[0m             store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[1;32m   3105\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m   3106\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   3107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:995\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001b[0m\n\u001b[1;32m    990\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39mopen(path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    991\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m         \u001b[39m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[1;32m    993\u001b[0m         \u001b[39m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[1;32m    994\u001b[0m         \u001b[39m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[0;32m--> 995\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39;49mopen_output_stream(\n\u001b[1;32m    996\u001b[0m             path, compression\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    997\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     sink \u001b[39m=\u001b[39m where\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pyarrow/_fs.pyx:868\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_output_stream\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/development/thesis/.venv/lib/python3.9/site-packages/pyarrow/error.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Failed to open local file '/Users/fahad/Desktop/processed_data/combined_data9.parquet'. Detail: [errno 21] Is a directory"
     ]
    }
   ],
   "source": [
    "folder = combine_parquet(folder_path, output_file_path, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "parent_folder = '/Users/fahad/Desktop/Data/More data'\n",
    "folders_without_files = []\n",
    "folders_with_files = {}\n",
    "\n",
    "# Loop through subfolders\n",
    "for folder_name in os.listdir(parent_folder):\n",
    "    folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        # Check if there are any files in the folder\n",
    "        if len(files) == 0:\n",
    "            folders_without_files.append(folder_name)\n",
    "        else:\n",
    "            folders_with_files[folder_name] = {\n",
    "                'file_count': len(files),\n",
    "                'files': files\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save folders without files to a text file\n",
    "with open('folders_without_files.txt', 'w') as file:\n",
    "    file.write(\"Folders without files:\\n\")\n",
    "    for folder_name in folders_without_files:\n",
    "        file.write(folder_name + '\\n')\n",
    "\n",
    "# Save folders with files to a text file\n",
    "with open('folders_with_files.txt', 'w') as file:\n",
    "    file.write(\"Folders with files:\\n\")\n",
    "    for folder_name, folder_info in folders_with_files.items():\n",
    "        file.write(f\"{folder_name} ({folder_info['file_count']} files):\\n\")\n",
    "        for file_name in folder_info['files']:\n",
    "            file.write(file_name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "parent_folder = 'More data'\n",
    "output_file = 'combined.parquet'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through the subfolders\n",
    "for root, dirs, files in os.walk(parent_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Read the parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Add the file path as a column\n",
    "            df['FilePath'] = file_path\n",
    "            \n",
    "            # Append the data to the combined DataFrame\n",
    "            combined_data = combined_data.append(df)\n",
    "\n",
    "# Save the combined data as a parquet file\n",
    "combined_data.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"Combined parquet file saved as '{output_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
